{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5545a4b",
   "metadata": {},
   "source": [
    "# 랩미팅\n",
    "goal.\n",
    "1. Qtable을 dynamic하게 정의\n",
    "2. Qtable을 학습시킨후 score에 대한 equation들을 출력\n",
    "--- 현재 ---\n",
    "\n",
    "3. 출력된 equation을 가지고 maude meta-level로 휴리스틱 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94394fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import maude\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323986d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DP5 module\n"
     ]
    }
   ],
   "source": [
    "maude.init()\n",
    "maude.load('./dp.maude')\n",
    "m = maude.getCurrentModule()\n",
    "print('Using', m, 'module')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e082279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p(0, hungry) || c(0) || p(1, eat) || p(2, think) || p(3, single) || p(4, think) || c(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dp_generator():\n",
    "    N = random.choice([5])\n",
    "    P = [0] * N #np.zeros(N, dtype=int)\n",
    "    C = [1] * N #np.ones(N, dtype=int)\n",
    "\n",
    "    for i in range(N):\n",
    "        c = random.randrange(3)\n",
    "        if c == 1:\n",
    "            # to left\n",
    "            P[(i-1) % N] += 1\n",
    "            C[i] = 0\n",
    "        elif c == 2:\n",
    "            # to right:\n",
    "            P[i] += 1\n",
    "            C[i] = 0\n",
    "\n",
    "    # here, self.P[i] denotes the number of chopstics assigned for ith philos\n",
    "    for i in range(N):\n",
    "        if P[i] == 0:\n",
    "            P[i] = random.randrange(2) # either think or hungry\n",
    "        else:\n",
    "            P[i] += 1 # one chopstick or eat\n",
    "    \n",
    "    s = []\n",
    "    for i in range(N):\n",
    "        if P[i] == 0:\n",
    "            s.append(f'p({i},think)')\n",
    "        elif P[i] == 1:\n",
    "            s.append(f'p({i},hungry)')\n",
    "        elif P[i] == 2:\n",
    "            s.append(f'p({i},single)')\n",
    "        elif P[i] == 3:\n",
    "            s.append(f'p({i},eat)')\n",
    "            \n",
    "        if C[i] == 0:\n",
    "            pass\n",
    "        elif C[i] == 1:\n",
    "            s.append(f'c({i})')\n",
    "    return m.parseTerm(' || '.join(s))\n",
    "\n",
    "dp_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d229763",
   "metadata": {},
   "source": [
    "MaudeEnv does not know anything about the model in consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5113ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaudeEnv():\n",
    "    def __init__(self, g, init_term=None):\n",
    "        self.conf_gen = g\n",
    "        self.reset(init_term)\n",
    "        \n",
    "    def reset(self, init_term=None):\n",
    "        if init_term == None:\n",
    "            t = self.conf_gen()\n",
    "        else:\n",
    "            t = init_term\n",
    "        self.term = t\n",
    "        self.acfg = self.get_acfg(t)\n",
    "        self.nbrs = [(t,self.get_acfg(t)) for t,_,_,_ in t.search(1, m.parseTerm('X:Conf'), depth = 1)]  \n",
    "        self.next_acfg = list(set([action for (_,action) in self.nbrs])) # remove duplicates\n",
    "        return self.get_state() \n",
    "    \n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'term' : self.term,\n",
    "            'acfg' : self.acfg,\n",
    "            'nbrs' : self.nbrs,\n",
    "            'next_acfg' : self.next_acfg,\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        pairs = [(term, acfg) for (term,acfg) in self.nbrs if acfg == action]\n",
    "        if pairs == []:\n",
    "            raise Exception(\"invalid action\")\n",
    "        state = self.reset(random.choice(pairs)[0])\n",
    "        done = True if self.nbrs == [] else False\n",
    "        reward = 1 if done else 0\n",
    "        return state, reward, done\n",
    "    \n",
    "    def get_acfg(self, t):\n",
    "        acfg = m.parseTerm('aconf(' + t.prettyPrint(0) + ')') # TODO change aconf to acfg\n",
    "        acfg.reduce()\n",
    "        return acfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d1d0c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable():\n",
    "    def __init__(self):\n",
    "        self.q_init = 0\n",
    "        self.q_dict = dict()\n",
    "        \n",
    "    def get_q(self, t1, t2):\n",
    "        q_init = self.q_init\n",
    "        #row = self.q_dict.get(t1, None)\n",
    "        #if not row == None:\n",
    "        if t1 in self.q_dict:\n",
    "            return self.q_dict[t1].get(t2, q_init)\n",
    "        return q_init\n",
    "        \n",
    "    def set_q(self, t1, t2, q):\n",
    "        # TODO deepcopy terms\n",
    "        if not t1 in self.q_dict:\n",
    "            self.q_dict[t1] = { t2 : q }\n",
    "        else:\n",
    "            self.q_dict[t1][t2] = q\n",
    "        \n",
    "    def argmax_q(self, t1, nbrs): # nbrs: iterable if acfg's\n",
    "        q_dict = self.q_dict\n",
    "        if t1 in q_dict and len(nbrs) != 0:\n",
    "            d = { t2 : q_dict[t1].get(t2, 0) for t2 in nbrs } # d = restriction of q_dict to tl\n",
    "            return max(d, key=d.get)\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def max_q(self, t):\n",
    "        q_dict = self.q_dict\n",
    "        if t in q_dict: # assume q_dict[t] is nonempty\n",
    "            return max(q_dict[t].values())\n",
    "        return 0\n",
    "    \n",
    "    def get_size(self):\n",
    "        # returns the number of nonzero entries in the QTable\n",
    "        ret = 0\n",
    "        for _, d in self.q_dict.items():\n",
    "            ret += len(d)\n",
    "        return ret\n",
    "    \n",
    "    def print_v(self):\n",
    "        q_dict = self.q_dict\n",
    "        print(f'fmod SCORE is')\n",
    "        for t in q_dict:\n",
    "            print(f'  eq score({t}) = {self.max_q(t)} .')\n",
    "        print(f'  eq score(X) = {self.q_init} [owise] .')\n",
    "        print(f'endfm')        \n",
    "    \n",
    "    def print_q(self):\n",
    "        q_dict = self.q_dict\n",
    "        print('load dp.maude')\n",
    "        print('mod SCORE is')\n",
    "        print('  pr DP5 .')\n",
    "        print('  pr FLOAT .')\n",
    "        print('  op score : AConf AConf -> Float .')\n",
    "        for t1, d in q_dict.items():\n",
    "            for t2, q in d.items():\n",
    "                print(f'  eq score({t1}, {t2}) = {q} .')\n",
    "        print(f'  eq score(X:AConf, Y:AConf) = {self.q_init} [owise] .') # TODO: 0 should be printed 0.0\n",
    "        print(f'endm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d367bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qt, state):\n",
    "    # Exploitation: take the action with the highest state, action value\n",
    "    t = state[\"acfg\"]\n",
    "    nbrs = state[\"next_acfg\"]\n",
    "    return Qt.argmax_q(t,nbrs)\n",
    "\n",
    "def eps_greedy_policy(Qtable, state, epsilon):\n",
    "    random_num = random.uniform(0, 1)\n",
    "    if random_num > epsilon: # exploitation\n",
    "        return greedy_policy(Qtable, state)\n",
    "    else: # exploration\n",
    "        nbrs = state[\"next_acfg\"]\n",
    "        if len(nbrs) != 0:\n",
    "            return random.choice(nbrs)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, qt):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            s = state[\"acfg\"]\n",
    "            a = eps_greedy_policy(qt, state, epsilon)\n",
    "            \n",
    "            # assert action not -1\n",
    "            if type(a) == type(-1):\n",
    "                break\n",
    "\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            #print('episode:', episode, 'step:', step, 'a:',a)\n",
    "            new_state, reward, done = env.step(a)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "\n",
    "            #Qtable[s][a] = Qtable[s][a] + learning_rate * (\n",
    "            #    reward + gamma * np.max(Qtable[a]) - Qtable[s][a]\n",
    "            #)\n",
    "            \n",
    "            new_q = qt.get_q(s, a) + learning_rate * (\n",
    "                reward + gamma * qt.max_q(a) - qt.get_q(s, a)\n",
    "            )\n",
    "            qt.set_q(s, a, new_q)\n",
    "\n",
    "            # If terminated or truncated finish the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "    print('training done!')\n",
    "    return qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9f76ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 100  # Total training episodes\n",
    "learning_rate = 0.7  # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100  # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "#env_id = \"FrozenLake-v1\"  # Name of the environment\n",
    "max_steps = 300  # Max steps per episode\n",
    "gamma = 0.95  # Discounting rate\n",
    "eval_seed = []  # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.05  # Minimum exploration probability\n",
    "decay_rate = 0.0005  # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4249e7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daed36475be8438fa13e843600743148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done!\n"
     ]
    }
   ],
   "source": [
    "# train Qtable\n",
    "env = MaudeEnv(dp_generator)\n",
    "Qtable = QTable()\n",
    "Qtable = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c2bcf3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dp.maude\n",
      "mod SCORE is\n",
      "  pr DP5 .\n",
      "  pr FLOAT .\n",
      "  op score : AConf AConf -> Float .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, hungry))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, think) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, hungry)), aconf(p(--, hungry) || p(--, single))) = 0.95 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, single))) = 0.95 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, single))) = 1.0 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, single))) = 0.95 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, think) || p(--, eat))) = 0.7737809374999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, hungry) || p(--, single))) = 0.95 .\n",
      "  eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, single))) = 0.9025 .\n",
      "  eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, eat))) = 0.7737809374999995 .\n",
      "  eq score(aconf(p(--, think) || p(--, eat)), aconf(p(--, think))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999 .\n",
      "  eq score(aconf(p(--, think) || p(--, eat)), aconf(p(--, think) || p(--, eat))) = 0.7737809365001294 .\n",
      "  eq score(aconf(p(--, think)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999 .\n",
      "  eq score(X:AConf, Y:AConf) = 0 [owise] .\n",
      "endm\n"
     ]
    }
   ],
   "source": [
    "Qtable.print_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "66abe47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, single))) = 0.95\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, single) || p(--, eat)), aconf(p(--, think) || p(--, eat))) = 0.7737809374999999\n",
      "eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, hungry) || p(--, single))) = 0.95\n",
      "eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, single)), aconf(p(--, think) || p(--, eat))) = 0.7737809374999999\n",
      "eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, single))) = 1.0\n",
      "eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, single))) = 0.95\n",
      "eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, hungry) || p(--, single)), aconf(p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, think) || p(--, hungry) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, hungry))) = 0.9025\n",
      "eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, think) || p(--, hungry)), aconf(p(--, think) || p(--, single))) = 0.9025\n",
      "eq score(aconf(p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, hungry) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, hungry) || p(--, eat)), aconf(p(--, hungry) || p(--, single) || p(--, eat))) = 0.8573749999999999\n",
      "eq score(aconf(p(--, hungry)), aconf(p(--, hungry) || p(--, single))) = 0.95\n",
      "eq score(aconf(p(--, think) || p(--, eat)), aconf(p(--, think))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, eat)), aconf(p(--, think) || p(--, hungry) || p(--, eat))) = 0.8145062499999999\n",
      "eq score(aconf(p(--, think) || p(--, eat)), aconf(p(--, think) || p(--, eat))) = 0.7737809374999999\n",
      "eq score(aconf(p(--, think)), aconf(p(--, think) || p(--, hungry))) = 0.8573749999999999\n"
     ]
    }
   ],
   "source": [
    "qd = Qtable.q_dict\n",
    "s = 0\n",
    "for t1, d in qd.items():\n",
    "    for t2, q in d.items():\n",
    "        print(f'eq score({t1}, {t2}) = {q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2629b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next: aconf(p(--, think) || p(--, hungry) || p(--, single))\n",
      "result: 2\n",
      "next: aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))\n",
      "result: 3\n",
      "next: aconf(p(--, hungry) || p(--, single) || p(--, eat))\n",
      "result: 0\n",
      "max: 3\n",
      "argmax: -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for QTable\n",
    "\n",
    "env = MaudeEnv(dp_generator, m.parseTerm('c(2) || c(3) || p(0, single) || p(1, think) || p(2, hungry) || p(3, hungry) || p(4, eat)'))\n",
    "\n",
    "t = env.acfg\n",
    "t1 = m.parseTerm('aconf(p(--, think) || p(--, single) || p(--, hungry))') # hit\n",
    "t2 = m.parseTerm('aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))') # hit\n",
    "t3 = m.parseTerm('aconf(p(--, hungry) || p(--, single) || p(--, eat))') # miss\n",
    "t4 = m.parseTerm('aconf(p(--, think))') # invalid move\n",
    "t1.reduce()\n",
    "t2.reduce()\n",
    "t3.reduce()\n",
    "t4.reduce()\n",
    "\n",
    "qt = QTable()\n",
    "qt.set_q(t, t1, 2)\n",
    "qt.set_q(t, t2, 3)\n",
    "qt.set_q(t, t4, 1)\n",
    "\n",
    "#print(env.next_acfg)\n",
    "for next_t in env.next_acfg:\n",
    "    print('next:', next_t)\n",
    "    print('result:',qt.get_q(t,next_t))\n",
    "    \n",
    "print('max:', qt.max_q(t))\n",
    "print('argmax:', qt.argmax_q(t, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "107af254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next: aconf(p(--, think) || p(--, hungry) || p(--, single))\n",
      "result: 2\n",
      "next: aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))\n",
      "result: 3\n",
      "next: aconf(p(--, hungry) || p(--, single) || p(--, eat))\n",
      "result: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nd1 = Qtable[t]\\nd2 = dict.fromkeys(env.next_acfg, 0)\\n{**d1, **d2}\\nprint(Qtable[t])\\nprint(env.next_acfg)\\n{ k: Qtable[t].get(k, 42) for k in env.next_acfg }\\n\\nfor k in Qtable[t]:\\n    print(k, k.__hash__())\\n    \\nprint('===')\\n\\nfor k in env.next_acfg:\\n    print(k, k.__hash__())\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MaudeEnv(dp_generator, m.parseTerm('c(2) || c(3) || p(0, single) || p(1, think) || p(2, hungry) || p(3, hungry) || p(4, eat)'))\n",
    "\n",
    "#print(env.get_state())\n",
    "\n",
    "t = env.acfg\n",
    "t1 = m.parseTerm('aconf(p(--, think) || p(--, hungry) || p(--, single))') # hit\n",
    "t2 = m.parseTerm('aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))') # hit\n",
    "t3 = m.parseTerm('aconf(p(--, hungry) || p(--, single) || p(--, eat))') # miss\n",
    "t4 = m.parseTerm('aconf(p(--, think))') # invalid move\n",
    "t1.reduce()\n",
    "t2.reduce()\n",
    "t3.reduce()\n",
    "t4.reduce()\n",
    "Qtable = { t : { t1: 2, t2: 3, t4: 1} }\n",
    "\n",
    "for next_t in env.next_acfg:\n",
    "    print('next:', next_t)\n",
    "    print('result:',Qtable[t].get(next_t, 0))\n",
    "\n",
    "'''\n",
    "d1 = Qtable[t]\n",
    "d2 = dict.fromkeys(env.next_acfg, 0)\n",
    "{**d1, **d2}\n",
    "print(Qtable[t])\n",
    "print(env.next_acfg)\n",
    "{ k: Qtable[t].get(k, 42) for k in env.next_acfg }\n",
    "\n",
    "for k in Qtable[t]:\n",
    "    print(k, k.__hash__())\n",
    "    \n",
    "print('===')\n",
    "\n",
    "for k in env.next_acfg:\n",
    "    print(k, k.__hash__())\n",
    "'''\n",
    "\n",
    "\n",
    "#print(dict(Qtable[t], **))\n",
    "#max(Qtable[t], key=Qtable[t].get)\n",
    "#[ dict.get(k, my_default_value) for k in my_iterable ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196a8472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)) 14293894632505905964\n",
      "tt: aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)) 14293894632505905964\n",
      "ttt: aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat)) 1443846585\n",
      "equal: True\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "t = m.parseTerm('c(2) || c(3) || p(0, single) || p(1, think) || p(2, hungry) || p(3, hungry) || p(4, eat)')\n",
    "#print(t.__hash__())\n",
    "t = m.parseTerm('aconf(c(2) || c(3) || p(0, single) || p(1, think) || p(2, hungry) || p(3, hungry) || p(4, eat))')\n",
    "#print(t.__hash__())\n",
    "t.reduce()\n",
    "#print(t.__hash__())\n",
    "print('t:', t, t.__hash__())\n",
    "tt = m.parseTerm('aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))')\n",
    "tt.reduce()\n",
    "ttt = m.parseTerm('aconf(p(--, think) || p(--, hungry) || p(--, eat) || p(--, single))')\n",
    "print('tt:', tt, tt.__hash__())\n",
    "print('ttt:', ttt, ttt.__hash__())\n",
    "print('equal:', tt.equal(t))\n",
    "s = 'aconf(c(2) || c(3) || p(0, single) || p(1, think) || p(2, hungry) || p(3, hungry) || p(4, eat))'\n",
    "ss = 'aconf(p(--, think) || p(--, hungry) || p(--, single) || p(--, eat))'\n",
    "sss = 'aconf(p(--, think) || p(--, hungry) || p(--, eat) || p(--, single))'\n",
    "eq = m.parseTerm(ss + ' == ' + sss)\n",
    "eq.reduce()\n",
    "print(eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "944adc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "3315544020\n",
      "3315544020\n",
      "3364720003\n",
      "3364720003\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "t1 = m.parseTerm('aconf(p(--, think) || p(--, hungry) || p(--, single))')\n",
    "t2 = m.parseTerm('aconf(p(--, hungry) || p(--, think) || p(--, single))')\n",
    "t3 = m.parseTerm('p(0, think) || p(1, hungry) || p(2, single)')\n",
    "t4 = m.parseTerm('p(1, hungry) || p(0, think) || p(2, single)')\n",
    "print(t1.equal(t2))\n",
    "print(t3.equal(t4))\n",
    "print(t1.hash())\n",
    "print(t2.hash())\n",
    "print(t3.hash())\n",
    "print(t4.hash()) # why equal() returns false while hash vals are the same?\n",
    "my_dict = {}\n",
    "my_dict[t3] = 42\n",
    "print(my_dict[t4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cda681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_greedy_policy(Qtable, state):\n",
    "    # Exploitation: take the action with the highest state, action value\n",
    "    aidx = state[\"aidx\"]\n",
    "    mask = state[\"mask\"]\n",
    "    #mask = np.where(True, mask^1, mask) # flip 0 & 1\n",
    "    if 0 in mask:\n",
    "        masked_Q = ma.masked_array(Qtable[aidx][:], mask=mask)\n",
    "        action = np.argmax(masked_Q)\n",
    "    else:\n",
    "        action = -1 # deadlock\n",
    "    return action\n",
    "\n",
    "def abs_epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # Randomly generate a number between 0 and 1\n",
    "    random_num = random.uniform(0, 1)\n",
    "    # if random_num > greater than epsilon --> exploitation\n",
    "    if random_num > epsilon:\n",
    "        # Take the action with the highest value given a state\n",
    "        # np.argmax can be useful here\n",
    "        action = abs_greedy_policy(Qtable, state)\n",
    "    # else --> exploration\n",
    "    else:\n",
    "        nbrs = state[\"nbrs\"]\n",
    "        if nbrs != []:\n",
    "            action = random.choice(nbrs)[1]\n",
    "        else:\n",
    "            action = -1\n",
    "    return action\n",
    "\n",
    "def abs_train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            s = state[\"aidx\"]\n",
    "            a = abs_epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            \n",
    "            # assert action not -1\n",
    "            if a == -1:\n",
    "                break\n",
    "\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done = env.step(a)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "\n",
    "            Qtable[s][a] = Qtable[s][a] + learning_rate * (\n",
    "                reward + gamma * np.max(Qtable[a]) - Qtable[s][a]\n",
    "            )\n",
    "\n",
    "            # If terminated or truncated finish the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "    print('training done!')\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02eb5d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mWarning: \u001b[0m<standard input>, line 0: bad token \u001b[35maidx\u001b[0m.\n",
      "\u001b[31mWarning: \u001b[0m<standard input>, line 0: no parse for term.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reduce'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mMaudeEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdp_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m Qtable \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m      3\u001b[0m Qtable \u001b[38;5;241m=\u001b[39m abs_train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mMaudeEnv.__init__\u001b[0;34m(self, g, init_term)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf_gen \u001b[38;5;241m=\u001b[39m g\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_term\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mMaudeEnv.reset\u001b[0;34m(self, init_term)\u001b[0m\n\u001b[1;32m     15\u001b[0m     t \u001b[38;5;241m=\u001b[39m init_term\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterm \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aidx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#self.nbrs = [t for t,_,_,_ in t.search(1, m.parseTerm('X:Conf'), depth = 1)]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbrs \u001b[38;5;241m=\u001b[39m [(t,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_aidx(t)) \u001b[38;5;28;01mfor\u001b[39;00m t,_,_,_ \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;241m1\u001b[39m, m\u001b[38;5;241m.\u001b[39mparseTerm(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX:Conf\u001b[39m\u001b[38;5;124m'\u001b[39m), depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mMaudeEnv.get_aidx\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_aidx\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[1;32m     44\u001b[0m     aidx \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mparseTerm(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maidx(\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m t\u001b[38;5;241m.\u001b[39mprettyPrint(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[43maidx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aidx\u001b[38;5;241m.\u001b[39mtoInt()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reduce'"
     ]
    }
   ],
   "source": [
    "env = MaudeEnv(dp_generator)\n",
    "Qtable = np.zeros(shape=(16,16))\n",
    "Qtable = abs_train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('absQtable density:', np.count_nonzero(Qtable), '/', Qtable.size)\n",
    "print(Qtable.shape)\n",
    "print(Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-greedy Simulator\n",
    "def abs_simulate(Qtable, max_steps=1000, epsilon=0.01):\n",
    "    state = env.reset()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Choose the action At using epsilon greedy policy\n",
    "        action = abs_epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "        if action == -1:\n",
    "            break\n",
    "\n",
    "        # Take action At and observe Rt+1 and St+1\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # If terminated or truncated finish the episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Our next state is the new state\n",
    "        state = new_state\n",
    "        \n",
    "    return step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000\n",
    "res_trained = []\n",
    "res_random = []\n",
    "for _ in tqdm(range(100)):\n",
    "    num_steps_trained = abs_simulate(Qtable, max_steps=max_steps, epsilon=0)\n",
    "    num_steps_random = abs_simulate(Qtable, max_steps=max_steps, epsilon=1)\n",
    "    res_trained.append(num_steps_trained)\n",
    "    res_random.append(num_steps_random)\n",
    "print('trained:')\n",
    "print(res_trained)\n",
    "print('avg:')\n",
    "print(np.average(res_trained))\n",
    "print('=====')\n",
    "print('random:')\n",
    "print(res_random)\n",
    "print('avg:')\n",
    "print(np.average(res_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e50ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MaudeEnv(dp_generator)\n",
    "env.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable = np.zeros(shape=(16,16))\n",
    "print(Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = m.parseTerm(\"p(0,think) || c(0) || p(s(0),think) || c(s(0)) || p(s(s(0)),think) || c(s(s(0)))\")\n",
    "env = MaudeEnv(dp_generator)\n",
    "\n",
    "while env.available_actions != []:\n",
    "    print('term:', env.term)\n",
    "    #print('aidx:', env.aidx)\n",
    "    action = random.choice(env.available_actions)\n",
    "    print('action (next aidx):', action)\n",
    "    state, reward, done = env.step(action)\n",
    "    print(f'action: {action}, state: {state}, reward: {reward}, done: {done}')\n",
    "    print('===')\n",
    "print('last term:', env.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635332df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tterm = m.parseTerm('p(0, single) || p(1, hungry) || p(2, eat) || p(3, hungry) || p(4, eat)')\n",
    "print(tterm)\n",
    "tenv = MaudeEnv(dp_generator, tterm)\n",
    "state = tenv.reset(tterm)\n",
    "#print(state)\n",
    "a = abs_greedy_policy(Qtable, state)\n",
    "print(a)\n",
    "state, reward, done = tenv.step(a)\n",
    "print(state, done)\n",
    "#print(abs_epsilon_greedy_policy(Qtable, state, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8d4ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current term: p(0, hungry) || c(0) || p(1, eat) || p(2, think) || p(3, single) || p(4, think) || c(4)\n",
      "\n",
      "=== Action 0 of applying rule [th] ===\n",
      "< RULE: rl [th] : p(I:Nat, think) => p(I:Nat, hungry) ., SUBSTITUTION: I:Nat=2 >\n",
      "--- abstract action --->\n",
      "< RULE: rl [th] : p(I:Nat, think) => p(I:Nat, hungry) ., SUBSTITUTION: I:One=unit >\n",
      "\n",
      "=== Action 1 of applying rule [th] ===\n",
      "< RULE: rl [th] : p(I:Nat, think) => p(I:Nat, hungry) ., SUBSTITUTION: I:Nat=4 >\n",
      "--- abstract action --->\n",
      "< RULE: rl [th] : p(I:Nat, think) => p(I:Nat, hungry) ., SUBSTITUTION: I:One=unit >\n"
     ]
    }
   ],
   "source": [
    "t = m.parseTerm('p(0, hungry) || c(0) || p(1, eat) || p(2, think) || p(3, single) || p(4, think) || c(4)')\n",
    "v = m.parseTerm('C:Conf')\n",
    "print('current term:',t)\n",
    "i = 0\n",
    "for r, sb, ctx, rl in t.apply('th'):\n",
    "    print('')\n",
    "    print(f'=== Action {i} of applying rule [th] ===')\n",
    "    print(f'< RULE: {rl}, SUBSTITUTION: {sb} >')\n",
    "    print(f'--- abstract action --->')\n",
    "    asb = ''\n",
    "    for (var,term) in sb:\n",
    "        sort = str(var.getSort())\n",
    "        alpha = 'alpha-' + sort # alpha-Nat\n",
    "        aterm = m.parseTerm(f'{alpha}({str(term)})') # alpha-Nat(term)\n",
    "        aterm.reduce() # unit\n",
    "        asb += ('I:One=' + str(aterm))\n",
    "    print(f'< RULE: {rl}, SUBSTITUTION: {asb} >')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48c7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
