{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d91a737",
   "metadata": {},
   "source": [
    "# Start\n",
    "We use dining philosophers model in this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc94e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HEURISTIC-SEARCH-DP module\n",
      "==========================================\n",
      "reduce in HEURISTIC-SEARCH-DP : abst(p(0, single) || p(1, single) || p(2, hungry)) .\n",
      "rewrites: 5 in 0ms cpu (0ms real) (~ rewrites/second)\n",
      "result AState: p(--, hungry) || p(--, single)\n"
     ]
    }
   ],
   "source": [
    "import maude\n",
    "\n",
    "model = './benchmarks/dining-philosophers/hs-dp.maude'\n",
    "maude.init()\n",
    "maude.load(model)\n",
    "m = maude.getCurrentModule()\n",
    "print('Using', m, 'module')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ea128",
   "metadata": {},
   "source": [
    "# Environment\n",
    "the maude model of dining philosophers is simulated by the MaudeEnv object.\n",
    "To create MaudeEnv for the DP maude model, you need to provide:\n",
    "* the maude module\n",
    "* a initializer function that returns a string of a term for initial states\n",
    "* the goal proposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7348fe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p(0,single) || p(1,think) || c(1) || p(2,eat) || p(3,hungry) || p(4,hungry) || c(4)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def dp_sampler(N):\n",
    "    #N = random.choice([5])\n",
    "    P = [0] * N #np.zeros(N, dtype=int)\n",
    "    C = [1] * N #np.ones(N, dtype=int)\n",
    "\n",
    "    for i in range(N):\n",
    "        c = random.randrange(3)\n",
    "        if c == 1:\n",
    "            # to left\n",
    "            P[(i-1) % N] += 1\n",
    "            C[i] = 0\n",
    "        elif c == 2:\n",
    "            # to right:\n",
    "            P[i] += 1\n",
    "            C[i] = 0\n",
    "\n",
    "    # here, self.P[i] denotes the number of chopstics assigned for ith philos\n",
    "    for i in range(N):\n",
    "        if P[i] == 0:\n",
    "            P[i] = random.randrange(2) # either think or hungry\n",
    "        else:\n",
    "            P[i] += 1 # one chopstick or eat\n",
    "    \n",
    "    s = []\n",
    "    for i in range(N):\n",
    "        if P[i] == 0:\n",
    "            s.append(f'p({i},think)')\n",
    "        elif P[i] == 1:\n",
    "            s.append(f'p({i},hungry)')\n",
    "        elif P[i] == 2:\n",
    "            s.append(f'p({i},single)')\n",
    "        elif P[i] == 3:\n",
    "            s.append(f'p({i},eat)')\n",
    "            \n",
    "        if C[i] == 0:\n",
    "            pass\n",
    "        elif C[i] == 1:\n",
    "            s.append(f'c({i})')\n",
    "    return ' || '.join(s)\n",
    "\n",
    "dp_sampler(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf57737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': c(0) || c(3) || c(4) || p(0, hungry) || p(1, single) || p(2, single) || p(3, think) || p(4, hungry), 'astate': p(--, think) || p(--, hungry) || p(--, single), 'actions': [<label: th, asubs: {I: --}>, <label: hs, asubs: {I: --, J:Nat: --}>, <label: hs, asubs: {I: --, J:Nat: --}>, <label: hs, asubs: {I: --, J:Nat: --}>, <label: se, asubs: {I: --, J:Nat: --}>]}\n"
     ]
    }
   ],
   "source": [
    "from AGCEL.MaudeEnv import MaudeEnv\n",
    "\n",
    "env = MaudeEnv(m,'deadlock',lambda : dp_sampler(5))\n",
    "print(env.get_obs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e60b6",
   "metadata": {},
   "source": [
    "# Training\n",
    "train using the train() method from QLearner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd98f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done!\n",
      "28\n",
      "{p(--, hungry) || p(--, single): {<label: hs, asubs: {I: --, J:Nat: --}>: 0.9729310918170474, <label: se, asubs: {I: --, J:Nat: --}>: 0.7860750035712095}, p(--, think) || p(--, single): {<label: th, asubs: {I: --}>: 0.8587932748646855, <label: se, asubs: {I: --, J:Nat: --}>: 0.7753329365589599}, p(--, think) || p(--, hungry) || p(--, single): {<label: hs, asubs: {I: --, J:Nat: --}>: 0.7972491866423093, <label: th, asubs: {I: --}>: 0.8798508900446251, <label: se, asubs: {I: --, J:Nat: --}>: 0.7736724988644834}, p(--, think) || p(--, single) || p(--, eat): {<label: et, asubs: {I: --}>: 0.8238105311243668, <label: th, asubs: {I: --}>: 0.7828822743570798, <label: se, asubs: {I: --, J:Nat: --}>: 0.7712825643146353}, p(--, think) || p(--, hungry) || p(--, single) || p(--, eat): {<label: et, asubs: {I: --}>: 0.8254719747057041, <label: hs, asubs: {I: --, J:Nat: --}>: 0.8090148698323536, <label: th, asubs: {I: --}>: 0.7931921199971858, <label: se, asubs: {I: --, J:Nat: --}>: 0.7736283527675254}, p(--, hungry) || p(--, single) || p(--, eat): {<label: et, asubs: {I: --}>: 0.8344867897089431, <label: se, asubs: {I: --, J:Nat: --}>: 0.7639212778055214, <label: hs, asubs: {I: --, J:Nat: --}>: 0.7706963261896091}, p(--, think) || p(--, hungry): {<label: hs, asubs: {I: --, J:Nat: --}>: 0.8437957027550992, <label: th, asubs: {I: --}>: 0.7186380100938008}, p(--, think) || p(--, hungry) || p(--, eat): {<label: hs, asubs: {I: --, J:Nat: --}>: 0.7192488838122069, <label: et, asubs: {I: --}>: 0.7489843875685313, <label: th, asubs: {I: --}>: 0.6855634621251814}, p(--, hungry) || p(--, eat): {<label: et, asubs: {I: --}>: 0.7459430644372659, <label: hs, asubs: {I: --, J:Nat: --}>: 0.7738353093827647}, p(--, think) || p(--, eat): {<label: th, asubs: {I: --}>: 0.6873870544414403, <label: et, asubs: {I: --}>: 0.670401578664092}, p(--, think): {<label: th, asubs: {I: --}>: 0.7640329863310691}, p(--, hungry): {<label: hs, asubs: {I: --, J:Nat: --}>: 0.759612674462336}}\n"
     ]
    }
   ],
   "source": [
    "from AGCEL.QLearning import QLearner\n",
    "\n",
    "learner = QLearner()\n",
    "stat = learner.train(env, 100)\n",
    "print(learner.get_size())\n",
    "print(learner.q_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbc099",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Compare the number of states explored between QHS vs BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa40d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal reached!\n",
      "goal reached!\n",
      "26 232\n",
      "goal reached!\n",
      "goal reached!\n",
      "13 254\n",
      "goal reached!\n",
      "goal reached!\n",
      "11 114\n",
      "goal reached!\n",
      "goal reached!\n",
      "40 951\n",
      "goal reached!\n",
      "goal reached!\n",
      "12 128\n"
     ]
    }
   ],
   "source": [
    "from AGCEL.HeuristicSearch import HeuristicSearch\n",
    "\n",
    "for _ in range(5):\n",
    "    env = HeuristicSearch(m,'deadlock',lambda : dp_sampler(5), learner)\n",
    "    qhs_result = env.search(mode='qhs')\n",
    "    env.reset(env.last_init)\n",
    "    bfs_result = env.search(mode='bfs')\n",
    "    print(qhs_result, bfs_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0281d63",
   "metadata": {},
   "source": [
    "### ------------- IGNORE BELOW ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d81c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class TermWrapper():\n",
    "    def __init__(self,t):\n",
    "        self.t = t\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return 0\n",
    "\n",
    "class HeuristicSearch(MaudeEnv):\n",
    "    def __init__(self, m, goal, initializer, qt):\n",
    "        MaudeEnv.__init__(self, m, goal, initializer)\n",
    "        self.qt = qt\n",
    "        self.last_init = self.state\n",
    "        \n",
    "    def get_nbrs(self):\n",
    "        #returns (next state, action) where action is applied to the current state to produce next state\n",
    "        return [(t, path()[1].getLabel()) for t, subs, path, nrew in self.state.search(1, self.m.parseTerm('X:State'), depth = 1)]\n",
    "    \n",
    "    def score(self, s, a):\n",
    "        astate = self.abst(s)\n",
    "        return self.qt.get_q(astate, a)\n",
    "        \n",
    "    def search(self, mode='qhs', max_step=1000):\n",
    "        visited = set()\n",
    "        i = 0\n",
    "        queue = [(i,TermWrapper(self.state))] # (priority, concrete_state)\n",
    "\n",
    "        while not queue == [] and i < max_step:\n",
    "            state = heapq.heappop(queue)[1].t\n",
    "            obs = self.reset(state)\n",
    "            #print(t)\n",
    "            if state in visited:\n",
    "                continue\n",
    "            i += 1\n",
    "            visited.add(state)\n",
    "            s = obs['astate']\n",
    "            if self.is_goal():\n",
    "                print('goal reached!')\n",
    "                #print('t:', t)\n",
    "                #print('num steps:', i)\n",
    "                break\n",
    "            # nbrs = [(v, av) for (a, v, av) in env.next_actions if not v in visited] # unvisited next vecs\n",
    "            if mode == 'bfs': # bfs\n",
    "                q_items = [(i, TermWrapper(next_state)) for (next_state, a) in self.get_nbrs()]\n",
    "            elif mode == 'qhs': # qhs\n",
    "                q_items = [(-self.score(state, a), TermWrapper(next_state)) for (next_state, a) in self.get_nbrs()] # prioritized nbrs\n",
    "            for item in q_items:\n",
    "                heapq.heappush(queue, item) # queue,item\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46d9e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self):\n",
    "        self.q_init = 0.0\n",
    "        self.q_dict = dict()\n",
    "        \n",
    "    def get_q(self, s, a):\n",
    "        q_init = self.q_init\n",
    "        if s in self.q_dict:\n",
    "            return self.q_dict[s].get(a, q_init)\n",
    "        return q_init\n",
    "        \n",
    "    def set_q(self, s, a, q):\n",
    "        # TODO deepcopy terms\n",
    "        if q == 0.0: # TODO\n",
    "            return\n",
    "        elif not s in self.q_dict:\n",
    "            self.q_dict[s] = { a : q }\n",
    "        else:\n",
    "            self.q_dict[s][a] = q\n",
    "        \n",
    "    def argmax_q(self, s, actions): # nbrs: iterable if acfg's\n",
    "        q_dict = self.q_dict\n",
    "        if s in q_dict and len(actions) != 0:\n",
    "            d = { a : q_dict[s].get(a, self.q_init) for a in actions } # d = restriction of q_dict to tl\n",
    "            return max(d, key=d.get) # FIXME: random choice if tie\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def max_q(self, s):\n",
    "        q_dict = self.q_dict\n",
    "        if s in q_dict: # assume q_dict[t] is nonempty\n",
    "            return max(q_dict[s].values())\n",
    "        return self.q_init\n",
    "    \n",
    "    def get_size(self):\n",
    "        # returns the number of nonzero entries in the QTable\n",
    "        ret = 0\n",
    "        for _, d in self.q_dict.items():\n",
    "            ret += len(d)\n",
    "        return ret\n",
    "    \n",
    "    def print_v(self):\n",
    "        q_dict = self.q_dict\n",
    "        print(f'fmod SCORE is')\n",
    "        for t in q_dict:\n",
    "            print(f'  eq score({t}) = {self.max_q(t)} .')\n",
    "        print(f'  eq score(X) = {self.q_init} [owise] .')\n",
    "        print(f'endfm')        \n",
    "    \n",
    "    def print_q(self):\n",
    "        q_dict = self.q_dict\n",
    "        print('load dp.maude')\n",
    "        print('mod SCORE is')\n",
    "        print('  pr DP5 .')\n",
    "        print('  pr FLOAT .')\n",
    "        print('  op score : AConf AConf -> Float .')\n",
    "        for t1, d in q_dict.items():\n",
    "            for t2, q in d.items():\n",
    "                print(f'  eq score({t1}, {t2}) = {q} .')\n",
    "        print(f'  eq score(X:AConf, Y:AConf) = {self.q_init} [owise] .') # TODO: 0 should be printed 0.0\n",
    "        print(f'endm')\n",
    "        \n",
    "    def greedy_policy(self, obs):\n",
    "        # returns -1 for error\n",
    "        astate = obs[\"astate\"]\n",
    "        actions = obs[\"actions\"]\n",
    "        return self.argmax_q(astate,actions)\n",
    "    \n",
    "    def eps_greedy_policy(self, obs, epsilon):\n",
    "        # returns -1 for error\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > epsilon: # exploitation\n",
    "            return self.greedy_policy(obs)\n",
    "        else: # exploration\n",
    "            actions = obs[\"actions\"]\n",
    "            if len(actions) != 0:\n",
    "                return random.choice(actions)\n",
    "            else:\n",
    "                return -1\n",
    "            \n",
    "    def train(self, env, n_training_episodes, min_epsilon, max_epsilon, decay_rate, max_steps):\n",
    "        stat = 0\n",
    "        for episode in tqdm(range(n_training_episodes)):\n",
    "            # Reduce epsilon (because we need less and less exploration)\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\n",
    "            obs = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                s = obs[\"astate\"]\n",
    "                a = self.eps_greedy_policy(obs, epsilon)\n",
    "\n",
    "                # assert action not -1\n",
    "                if type(a) == type(-1):\n",
    "                    break\n",
    "\n",
    "                obs, reward, done = env.step(a)\n",
    "                ns = obs['astate']\n",
    "                stat += reward\n",
    "\n",
    "                # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "                nq = self.get_q(s, a) + learning_rate * (\n",
    "                    reward + gamma * self.max_q(ns) - self.get_q(s, a) # FIXME!!!!! max_q(s')!!!!\n",
    "                )\n",
    "                self.set_q(s, a, nq)\n",
    "\n",
    "                # If terminated or truncated finish the episode\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        print('training done!')\n",
    "        return self.q_dict, stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b61b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def greedy_policy(Qt, obs):\n",
    "    # Exploitation: take the action with the highest state, action value\n",
    "    astate = obs[\"astate\"]\n",
    "    actions = obs[\"actions\"]\n",
    "    return Qt.argmax_q(astate,actions)\n",
    "\n",
    "def eps_greedy_policy(Qtable, obs, epsilon):\n",
    "    r = random.uniform(0, 1)\n",
    "    if r > epsilon: # exploitation\n",
    "        return greedy_policy(Qtable, obs)\n",
    "    else: # exploration\n",
    "        actions = obs[\"actions\"]\n",
    "        if len(actions) != 0:\n",
    "            return random.choice(actions)\n",
    "        else:\n",
    "            return -1\n",
    "'''\n",
    "        \n",
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, qt):\n",
    "    stat = 0\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # Reset the environment\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            s = obs[\"astate\"]\n",
    "            #a = eps_greedy_policy(qt, obs, epsilon)\n",
    "            a = qt.eps_greedy_policy(obs, epsilon)\n",
    "            \n",
    "            # assert action not -1\n",
    "            if type(a) == type(-1):\n",
    "                break\n",
    "\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            #print('episode:', episode, 'step:', step, 'a:',a)\n",
    "            obs, reward, done = env.step(a)\n",
    "            ns = obs['astate']\n",
    "            stat += reward\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "\n",
    "            #Qtable[s][a] = Qtable[s][a] + learning_rate * (\n",
    "            #    reward + gamma * np.max(Qtable[a]) - Qtable[s][a]\n",
    "            #)\n",
    "            \n",
    "            nq = qt.get_q(s, a) + learning_rate * (\n",
    "                reward + gamma * qt.max_q(ns) - qt.get_q(s, a) # FIXME!!!!! max_q(s')!!!!\n",
    "            )\n",
    "            qt.set_q(s, a, nq)\n",
    "\n",
    "            # If terminated or truncated finish the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    print('training done!')\n",
    "    return qt, stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39db8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 100  # Total training episodes\n",
    "learning_rate = 0.7  # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100  # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "#env_id = \"FrozenLake-v1\"  # Name of the environment\n",
    "max_steps = 300  # Max steps per episode\n",
    "gamma = 0.95  # Discounting rate\n",
    "eval_seed = []  # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.05  # Minimum exploration probability\n",
    "decay_rate = 0.0005  # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "963aea9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8843b431b55c4042a1bfd45e5d1339c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done!\n"
     ]
    }
   ],
   "source": [
    "# train Qtable\n",
    "#env = MaudeEnv(dp_generator)\n",
    "env = MaudeEnv(m,'deadlock',dp_generator)\n",
    "Qtable = QTable()\n",
    "Qtable = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b1a4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt=Qtable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e223bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{p(--, hungry) || p(--, single): {'hs': 0.986145285034768,\n",
       "  'se': 0.7278900926619839},\n",
       " p(--, think) || p(--, single): {'th': 0.8075372104341592,\n",
       "  'se': 0.7562435959691568},\n",
       " p(--, think) || p(--, single) || p(--, eat): {'et': 0.7965121647914764,\n",
       "  'th': 0.7148195844801836,\n",
       "  'se': 0.7363522087237413},\n",
       " p(--, think) || p(--, hungry) || p(--, single) || p(--, eat): {'hs': 0.7487671188435407,\n",
       "  'et': 0.7501174529798867,\n",
       "  'th': 0.7257537928562119,\n",
       "  'se': 0.6708908792568634},\n",
       " p(--, think) || p(--, hungry) || p(--, single): {'se': 0.7050434538998628,\n",
       "  'th': 0.7843602857822237,\n",
       "  'hs': 0.7759559170731525},\n",
       " p(--, think) || p(--, hungry) || p(--, eat): {'hs': 0.7111647379228591,\n",
       "  'th': 0.6962760899866769,\n",
       "  'et': 0.6918448527266022},\n",
       " p(--, think) || p(--, hungry): {'hs': 0.7806181721044272,\n",
       "  'th': 0.7147510240924144},\n",
       " p(--, hungry) || p(--, eat): {'et': 0.6831780357946137,\n",
       "  'hs': 0.7952513472362125},\n",
       " p(--, hungry) || p(--, single) || p(--, eat): {'et': 0.7457844198602055,\n",
       "  'se': 0.7316412739652933,\n",
       "  'hs': 0.752006754276142},\n",
       " p(--, hungry): {'hs': 0.8945227923957144},\n",
       " p(--, think): {'th': 0.7294751660617126},\n",
       " p(--, think) || p(--, eat): {'th': 0.7179036767327655,\n",
       "  'et': 0.6952135956892009}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt.q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048c325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46997025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable():\n",
    "    def __init__(self):\n",
    "        self.q_init = 0.0\n",
    "        self.q_dict = dict()\n",
    "        \n",
    "    def get_q(self, s, a):\n",
    "        q_init = self.q_init\n",
    "        if s in self.q_dict:\n",
    "            return self.q_dict[s].get(a, q_init)\n",
    "        return q_init\n",
    "        \n",
    "    def set_q(self, s, a, q):\n",
    "        # TODO deepcopy terms\n",
    "        if q == 0.0: # TODO\n",
    "            return\n",
    "        elif not s in self.q_dict:\n",
    "            self.q_dict[s] = { a : q }\n",
    "        else:\n",
    "            self.q_dict[s][a] = q\n",
    "        \n",
    "    def argmax_q(self, s, actions): # nbrs: iterable if acfg's\n",
    "        q_dict = self.q_dict\n",
    "        if s in q_dict and len(actions) != 0:\n",
    "            d = { a : q_dict[s].get(a, self.q_init) for a in actions } # d = restriction of q_dict to tl\n",
    "            return max(d, key=d.get) # FIXME: random choice if tie\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def max_q(self, s):\n",
    "        q_dict = self.q_dict\n",
    "        if s in q_dict: # assume q_dict[t] is nonempty\n",
    "            return max(q_dict[s].values())\n",
    "        return self.q_init\n",
    "    \n",
    "    def get_size(self):\n",
    "        # returns the number of nonzero entries in the QTable\n",
    "        ret = 0\n",
    "        for _, d in self.q_dict.items():\n",
    "            ret += len(d)\n",
    "        return ret\n",
    "    \n",
    "    def print_v(self):\n",
    "        q_dict = self.q_dict\n",
    "        print(f'fmod SCORE is')\n",
    "        for t in q_dict:\n",
    "            print(f'  eq score({t}) = {self.max_q(t)} .')\n",
    "        print(f'  eq score(X) = {self.q_init} [owise] .')\n",
    "        print(f'endfm')        \n",
    "    \n",
    "    def print_q(self):\n",
    "        q_dict = self.q_dict\n",
    "        print('load dp.maude')\n",
    "        print('mod SCORE is')\n",
    "        print('  pr DP5 .')\n",
    "        print('  pr FLOAT .')\n",
    "        print('  op score : AConf AConf -> Float .')\n",
    "        for t1, d in q_dict.items():\n",
    "            for t2, q in d.items():\n",
    "                print(f'  eq score({t1}, {t2}) = {q} .')\n",
    "        print(f'  eq score(X:AConf, Y:AConf) = {self.q_init} [owise] .') # TODO: 0 should be printed 0.0\n",
    "        print(f'endm')\n",
    "        \n",
    "    def greedy_policy(self, obs):\n",
    "        # returns -1 for error\n",
    "        astate = obs[\"astate\"]\n",
    "        actions = obs[\"actions\"]\n",
    "        return self.argmax_q(astate,actions)\n",
    "    \n",
    "    def eps_greedy_policy(self, obs, epsilon):\n",
    "        # returns -1 for error\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > epsilon: # exploitation\n",
    "            return greedy_policy(Qtable, obs)\n",
    "        else: # exploration\n",
    "            actions = obs[\"actions\"]\n",
    "            if len(actions) != 0:\n",
    "                return random.choice(actions)\n",
    "            else:\n",
    "                return -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
